{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JEyPFLTiTVh-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jason Christian\\anaconda3\\envs\\FaceNet\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "from scipy.spatial.distance import cosine\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mf_iU5RqYtZN"
      },
      "outputs": [],
      "source": [
        "image_path= './training_data/Brian Alexander/aug_0_WhatsApp Image 2025-09-21 at 11.14.19_355b0534.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2f33d86320184170b812f61dc6265702",
            "dcd4c2d6967e4da58bc7990237dd5804",
            "87a1b361715a483d9c563b323e95b4ce",
            "cbd9d041c87a4eb8980c7b42a46a8dd1",
            "30ef87bc803a4362b1666641f7101519",
            "a10ba5cc249a44649405db59b212ae7f",
            "e66df7252a2a4992b86dbb9aca693fcd",
            "4a779b0648ef44bbbd3b073849cea617",
            "90082bc963214a5c8e4f823e173a6265",
            "7ce610988fea45c7824d829f1f110a76",
            "589f696b730d4b8bbec70d67fa319b96"
          ]
        },
        "id": "MjyPiak-Ytv7",
        "outputId": "b609db3d-19a1-4604-9559-0388ead653cd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize MTCNN (face detection)\n",
        "mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "# Load the pretrained FaceNet model (feature extractor mode)\n",
        "model = InceptionResnetV1(pretrained='vggface2').to(device)\n",
        "\n",
        "# Freeze all layers initially\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last few layers for fine-tuning\n",
        "for param in model.block8.parameters():  # Last layers in FaceNet\n",
        "    param.requires_grad = True\n",
        "for param in model.last_linear.parameters():  # Final FC layer\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
        "criterion = torch.nn.MSELoss()  # Using MSE loss for embeddings fine-tuning\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QNHCcT74Zz8J",
        "outputId": "9cd794ea-6c9d-41ee-fefd-faa9e7c7e5f9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "# Make sure you have the MTCNN instance defined, e.g.:\n",
        "# from facenet_pytorch import MTCNN\n",
        "# mtcnn = MTCNN(keep_all=True)\n",
        "\n",
        "def detect_and_draw_faces(image_path):\n",
        "    \"\"\"\n",
        "    Detect faces in an image and draw bounding boxes.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not load image from {image_path}. Please check if the file exists and is a valid image.\")\n",
        "        return\n",
        "\n",
        "    # Convert to RGB for MTCNN detection\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Perform detection\n",
        "    # Ensure 'mtcnn' is defined globally or passed as an argument\n",
        "    detection_results = mtcnn.detect(img_rgb, landmarks=True)\n",
        "    \n",
        "    if detection_results is not None:\n",
        "        boxes, probs, landmarks = detection_results\n",
        "        \n",
        "        if boxes is not None:\n",
        "            for box in boxes:\n",
        "                # Draw rectangle\n",
        "                cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
        "        \n",
        "        # --- VS CODE ADJUSTMENT STARTS HERE ---\n",
        "        cv2.imshow(\"Detected Faces\", img)\n",
        "        \n",
        "        # specific to local execution: waits indefinitely for a key press\n",
        "        cv2.waitKey(0) \n",
        "        \n",
        "        # closes the window after the key is pressed\n",
        "        cv2.destroyAllWindows() \n",
        "        # --- VS CODE ADJUSTMENT ENDS HERE ---\n",
        "        \n",
        "    else:\n",
        "        print(\"No faces detected.\")\n",
        "\n",
        "# Example Usage:\n",
        "# detect_and_draw_faces('my_face.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hRI2VrdwcTA6"
      },
      "outputs": [],
      "source": [
        "def get_face_embeddings(image_path):\n",
        "    \"\"\"\n",
        "    Extracts face embeddings from an image.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not read image at {image_path}\")\n",
        "        return []\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    faces = mtcnn(img_rgb)\n",
        "    embeddings = []\n",
        "    if faces is not None:\n",
        "        for face in faces:\n",
        "            face = face.unsqueeze(0)\n",
        "            # Move the input tensor to the same device as the model\n",
        "            face = face.to(device)\n",
        "            # Set the model to evaluation mode to avoid using batch statistics during inference\n",
        "            model.eval()\n",
        "            embedding = model(face)\n",
        "            embeddings.append(embedding)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqmUb3nz-naH",
        "outputId": "6c4e2a69-dd0f-401f-9e6b-4cbf4537e912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Alex', 'Brian Alexander', 'Jason', 'Melvern', 'Sally']\n"
          ]
        }
      ],
      "source": [
        "dataset = './training_data/'\n",
        "print(os.listdir(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te1tfJb5AH_L",
        "outputId": "806baac9-19f1-407d-8b3e-0f545c824893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing images for: Alex\n",
            "Processing images for: Brian Alexander\n",
            "Processing images for: Jason\n",
            "Processing images for: Melvern\n",
            "No faces detected in ./training_data/Melvern\\aug_1_WhatsApp Image 2025-09-24 at 13.15.40_05b3ebc1.jpg, skipping.\n",
            "No faces detected in ./training_data/Melvern\\aug_9_WhatsApp Image 2025-09-24 at 13.15.40_b61f450f.jpg, skipping.\n",
            "No faces detected in ./training_data/Melvern\\aug_9_WhatsApp Image 2025-09-24 at 13.15.41_445b8df6.jpg, skipping.\n",
            "No faces detected in ./training_data/Melvern\\aug_9_WhatsApp Image 2025-09-24 at 13.15.42_ca1f80fc.jpg, skipping.\n",
            "Processing images for: Sally\n",
            "\n",
            "Face embedding extraction complete.\n",
            "Extracted embeddings for 5 individuals.\n"
          ]
        }
      ],
      "source": [
        "person_embeddings = {}\n",
        "\n",
        "# Iterate through each person's directory in the dataset\n",
        "for person_name in os.listdir(dataset):\n",
        "    person_dir = os.path.join(dataset, person_name)\n",
        "    if os.path.isdir(person_dir):\n",
        "        print(f\"Processing images for: {person_name}\")\n",
        "        embeddings_for_person = []\n",
        "        for image_file in os.listdir(person_dir):\n",
        "            if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "                image_path = os.path.join(person_dir, image_file)\n",
        "                # Get embeddings for all faces in the current image\n",
        "                current_image_embeddings = get_face_embeddings(image_path)\n",
        "                if current_image_embeddings:\n",
        "                    embeddings_for_person.extend(current_image_embeddings)\n",
        "                else:\n",
        "                    print(f\"No faces detected in {image_path}, skipping.\")\n",
        "        if embeddings_for_person:\n",
        "            person_embeddings[person_name] = embeddings_for_person\n",
        "        else:\n",
        "            print(f\"No embeddings found for {person_name}. This person might be skipped if no faces were detected in any of their images.\")\n",
        "\n",
        "print(\"\\nFace embedding extraction complete.\")\n",
        "print(f\"Extracted embeddings for {len(person_embeddings)} individuals.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5X2xPScRKCMI",
        "outputId": "38364176-29f1-4b37-e9ce-2cf126d86755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples for SVM: 67\n",
            "Total unique individuals: 5\n",
            "Training samples: 53\n",
            "Testing samples: 14\n",
            "\n",
            "SVM Model Accuracy: 1.0000\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "           Alex       1.00      1.00      1.00         1\n",
            "Brian Alexander       1.00      1.00      1.00         2\n",
            "          Jason       1.00      1.00      1.00         2\n",
            "        Melvern       1.00      1.00      1.00         8\n",
            "          Sally       1.00      1.00      1.00         1\n",
            "\n",
            "       accuracy                           1.00        14\n",
            "      macro avg       1.00      1.00      1.00        14\n",
            "   weighted avg       1.00      1.00      1.00        14\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Prepare data for SVM\n",
        "X = []  # Embeddings\n",
        "y = []  # Labels\n",
        "\n",
        "for person_name, embeddings_list in person_embeddings.items():\n",
        "    for embedding_tensor in embeddings_list:\n",
        "        # Detach the tensor from the GPU (if on GPU) and convert to numpy\n",
        "        embedding_np = embedding_tensor.detach().cpu().numpy().flatten()\n",
        "        X.append(embedding_np)\n",
        "        y.append(person_name)\n",
        "\n",
        "X = np.array(X)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"Total samples for SVM: {len(X)}\")\n",
        "print(f\"Total unique individuals: {len(label_encoder.classes_)}\")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "# Train SVM model\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = svm_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nSVM Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Optionally, you can print the trained SVM model details\n",
        "# print(\"\\nSVM Model:\", svm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e0_8emdFSpt5",
        "outputId": "fff2ee31-7c69-4176-bb4d-efa15070a4fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected face 1 in Alex1.jpg is: Alex\n",
            "Confidence: 0.35\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "from PIL import Image  # Required for Image.fromarray\n",
        "\n",
        "def recognize_person(image_path, model, label_encoder, mtcnn_detector, embedding_model):\n",
        "    # 1. Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not load image from {image_path}.\")\n",
        "        return\n",
        "\n",
        "    # Convert to RGB for MTCNN operations (MTCNN expects RGB)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 2. Detect faces\n",
        "    try:\n",
        "        detection_results = mtcnn_detector.detect(img_rgb, landmarks=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during face detection with landmarks: {e}\")\n",
        "        print(\"Attempting to detect without landmarks as fallback.\")\n",
        "        detection_results = mtcnn_detector.detect(img_rgb, landmarks=False)\n",
        "\n",
        "    if detection_results is not None and len(detection_results) >= 2:\n",
        "        boxes = detection_results[0]\n",
        "        \n",
        "        if boxes is not None and len(boxes) > 0:\n",
        "            # 3. Extract aligned faces\n",
        "            # Note: Ensure 'device' is defined globally in your script (e.g., device = 'cuda' or 'cpu')\n",
        "            faces_aligned = mtcnn_detector.extract(Image.fromarray(img_rgb), boxes, save_path=None)\n",
        "\n",
        "            if faces_aligned is not None and len(faces_aligned) > 0:\n",
        "                for i, face_tensor in enumerate(faces_aligned):\n",
        "                    # Prepare tensor for embedding model\n",
        "                    face_tensor = face_tensor.unsqueeze(0).to(device) \n",
        "                    embedding = embedding_model(face_tensor).detach().cpu().numpy().flatten()\n",
        "\n",
        "                    # Predict\n",
        "                    predicted_label_encoded = model.predict([embedding])\n",
        "                    predicted_person = label_encoder.inverse_transform(predicted_label_encoded)\n",
        "                    \n",
        "                    # Probability / Confidence\n",
        "                    probabilities = model.predict_proba([embedding])[0]\n",
        "                    confidence = probabilities[predicted_label_encoded[0]]\n",
        "                    \n",
        "                    print(f\"Detected face {i+1} in {os.path.basename(image_path)} is: {predicted_person[0]}\")\n",
        "                    print(f\"Confidence: {confidence:.2f}\")\n",
        "\n",
        "                    # Draw bounding box and label\n",
        "                    if i < len(boxes):\n",
        "                        box = boxes[i]\n",
        "                        # Drawing on img_rgb\n",
        "                        cv2.rectangle(img_rgb, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
        "                        cv2.putText(img_rgb, f\"{predicted_person[0]} ({confidence:.2f})\", \n",
        "                                    (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "                # --- VS CODE ADJUSTMENT START ---\n",
        "                # Convert RGB back to BGR for OpenCV display\n",
        "                final_display_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "                \n",
        "                # Create the window\n",
        "                cv2.imshow(\"Recognition Result\", final_display_img)\n",
        "                \n",
        "                # Wait indefinitely until a key is pressed\n",
        "                cv2.waitKey(0)\n",
        "                \n",
        "                # Close the window\n",
        "                cv2.destroyAllWindows()\n",
        "                # --- VS CODE ADJUSTMENT END ---\n",
        "\n",
        "            else:\n",
        "                print(f\"No aligned faces could be extracted from {os.path.basename(image_path)}.\")\n",
        "        else:\n",
        "            print(f\"No bounding boxes were detected for faces in {os.path.basename(image_path)}.\")\n",
        "    else:\n",
        "        print(f\"No faces detected or unexpected output in {os.path.basename(image_path)}.\")\n",
        "\n",
        "# Usage\n",
        "\n",
        "input_image_path = './training_data/Alex/Alex1.jpg'\n",
        "# Ensure these variables (input_image_path, svm_model, etc.) are defined above this function\n",
        "recognize_person(input_image_path, svm_model, label_encoder, mtcnn, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting live face recognition with tracking. Press 'q' to quit.\n",
            "Live face recognition stopped.\n",
            "Live face recognition stopped.\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "class FaceTracker:\n",
        "    \"\"\"\n",
        "    Tracks faces across frames to maintain stable bounding boxes and labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_distance=50, max_age=30):\n",
        "        self.tracks = {}  # {track_id: {'box': box, 'label': label, 'confidence': conf, 'age': age}}\n",
        "        self.next_id = 0\n",
        "        self.max_distance = max_distance  # Max distance to consider same face\n",
        "        self.max_age = max_age  # Frames to keep track alive without detection\n",
        "    \n",
        "    def distance(self, box1, box2):\n",
        "        \"\"\"Calculate distance between two bounding boxes (center to center).\"\"\"\n",
        "        x1_center = (box1[0] + box1[2]) / 2\n",
        "        y1_center = (box1[1] + box1[3]) / 2\n",
        "        x2_center = (box2[0] + box2[2]) / 2\n",
        "        y2_center = (box2[1] + box2[3]) / 2\n",
        "        return math.sqrt((x1_center - x2_center)**2 + (y1_center - y2_center)**2)\n",
        "    \n",
        "    def update(self, detections):\n",
        "        \"\"\"\n",
        "        Update tracks with new detections.\n",
        "        detections: list of {'box': box, 'label': label, 'confidence': conf}\n",
        "        \"\"\"\n",
        "        # Increment age of all tracks\n",
        "        for track_id in list(self.tracks.keys()):\n",
        "            self.tracks[track_id]['age'] += 1\n",
        "            if self.tracks[track_id]['age'] > self.max_age:\n",
        "                del self.tracks[track_id]\n",
        "        \n",
        "        # Match detections to existing tracks\n",
        "        used_detections = set()\n",
        "        \n",
        "        for track_id, track in self.tracks.items():\n",
        "            best_distance = float('inf')\n",
        "            best_detection_idx = -1\n",
        "            \n",
        "            for det_idx, detection in enumerate(detections):\n",
        "                if det_idx in used_detections:\n",
        "                    continue\n",
        "                \n",
        "                dist = self.distance(track['box'], detection['box'])\n",
        "                if dist < best_distance and dist < self.max_distance:\n",
        "                    best_distance = dist\n",
        "                    best_detection_idx = det_idx\n",
        "            \n",
        "            if best_detection_idx >= 0:\n",
        "                # Update track with new detection\n",
        "                detection = detections[best_detection_idx]\n",
        "                self.tracks[track_id]['box'] = detection['box']\n",
        "                self.tracks[track_id]['label'] = detection['label']\n",
        "                self.tracks[track_id]['confidence'] = detection['confidence']\n",
        "                self.tracks[track_id]['age'] = 0  # Reset age\n",
        "                used_detections.add(best_detection_idx)\n",
        "        \n",
        "        # Create new tracks for unmatched detections\n",
        "        for det_idx, detection in enumerate(detections):\n",
        "            if det_idx not in used_detections:\n",
        "                self.tracks[self.next_id] = {\n",
        "                    'box': detection['box'],\n",
        "                    'label': detection['label'],\n",
        "                    'confidence': detection['confidence'],\n",
        "                    'age': 0\n",
        "                }\n",
        "                self.next_id += 1\n",
        "        \n",
        "        return self.tracks\n",
        "\n",
        "\n",
        "def live_face_recognition(model, label_encoder, mtcnn_detector, embedding_model, confidence_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Performs live face recognition using webcam with object tracking.\n",
        "    Press 'q' to quit.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    \n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open camera.\")\n",
        "        return\n",
        "    \n",
        "    print(\"Starting live face recognition with tracking. Press 'q' to quit.\")\n",
        "    \n",
        "    # Set camera resolution for better performance\n",
        "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "    \n",
        "    frame_count = 0\n",
        "    detection_interval = 3  # Detect faces every 3 frames\n",
        "    tracker = FaceTracker(max_distance=60, max_age=20)\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"Error: Failed to read frame from camera.\")\n",
        "            break\n",
        "        \n",
        "        frame_count += 1\n",
        "        detections = []\n",
        "        \n",
        "        # Perform face detection every N frames\n",
        "        if frame_count % detection_interval == 0:\n",
        "            try:\n",
        "                # Convert BGR to RGB for processing\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                \n",
        "                # Detect faces\n",
        "                detection_results = mtcnn_detector.detect(frame_rgb, landmarks=False)\n",
        "                \n",
        "                if detection_results is not None and len(detection_results) >= 1:\n",
        "                    boxes = detection_results[0]\n",
        "                    \n",
        "                    if boxes is not None and len(boxes) > 0:\n",
        "                        # Extract aligned faces\n",
        "                        try:\n",
        "                            faces_aligned = mtcnn_detector.extract(Image.fromarray(frame_rgb), boxes, save_path=None)\n",
        "                            \n",
        "                            if faces_aligned is not None and len(faces_aligned) > 0:\n",
        "                                for i, face_tensor in enumerate(faces_aligned):\n",
        "                                    if i < len(boxes):\n",
        "                                        # Get embedding\n",
        "                                        face_tensor = face_tensor.unsqueeze(0).to(device)\n",
        "                                        embedding_model.eval()\n",
        "                                        embedding = embedding_model(face_tensor).detach().cpu().numpy().flatten()\n",
        "                                        \n",
        "                                        # Predict\n",
        "                                        predicted_label_encoded = model.predict([embedding])\n",
        "                                        predicted_person = label_encoder.inverse_transform(predicted_label_encoded)\n",
        "                                        \n",
        "                                        # Get confidence\n",
        "                                        probabilities = model.predict_proba([embedding])[0]\n",
        "                                        confidence = probabilities[predicted_label_encoded[0]]\n",
        "                                        \n",
        "                                        # Prepare detection\n",
        "                                        box = boxes[i]\n",
        "                                        label_text = f\"{predicted_person[0]} ({confidence:.2f})\"\n",
        "                                        if confidence < confidence_threshold:\n",
        "                                            label_text = f\"Unknown ({confidence:.2f})\"\n",
        "                                        \n",
        "                                        detections.append({\n",
        "                                            'box': box,\n",
        "                                            'label': label_text,\n",
        "                                            'confidence': confidence\n",
        "                                        })\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error during face extraction: {e}\")\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(f\"Error during frame processing: {e}\")\n",
        "        \n",
        "        # Update tracker with new detections\n",
        "        tracked_faces = tracker.update(detections)\n",
        "        \n",
        "        # Draw tracked faces on frame\n",
        "        for track_id, track in tracked_faces.items():\n",
        "            box = track['box']\n",
        "            label = track['label']\n",
        "            \n",
        "            # Determine color based on age (newer tracks are brighter)\n",
        "            brightness = max(50, 255 - (track['age'] * 10))\n",
        "            color = (0, int(brightness), 0) if 'Unknown' not in label else (0, 0, int(brightness))\n",
        "            \n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n",
        "            \n",
        "            # Draw label with background for better visibility\n",
        "            text = label\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            font_scale = 0.7\n",
        "            thickness = 2\n",
        "            \n",
        "            # Get text size for background\n",
        "            text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
        "            text_x = int(box[0])\n",
        "            text_y = int(box[1]) - 10\n",
        "            \n",
        "            # Draw background rectangle for text\n",
        "            cv2.rectangle(frame, (text_x, text_y - text_size[1] - 5), \n",
        "                         (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
        "            \n",
        "            # Draw text\n",
        "            cv2.putText(frame, text, (text_x, text_y), font, font_scale, (255, 255, 255), thickness)\n",
        "        \n",
        "        # Display the frame\n",
        "        cv2.imshow(\"Live Face Recognition with Tracking\", frame)\n",
        "        \n",
        "        # Press 'q' to quit\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    \n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Live face recognition stopped.\")\n",
        "\n",
        "# Run live face recognition with tracking\n",
        "live_face_recognition(svm_model, label_encoder, mtcnn, model, confidence_threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Dimension Check:\n",
            "==================================================\n",
            "Embedding dimension: 512\n",
            "Embedding shape: torch.Size([1, 512])\n",
            "SVM model expects input dimension: 512\n",
            "✓ Embedding and SVM dimensions match! (512D)\n"
          ]
        }
      ],
      "source": [
        "# Check embedding dimension\n",
        "print(\"Embedding Dimension Check:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get a sample embedding from the model\n",
        "sample_face = torch.randn(1, 3, 160, 160).to(device)  # Random face tensor (batch_size=1, channels=3, height=160, width=160)\n",
        "with torch.no_grad():\n",
        "    sample_embedding = model(sample_face)\n",
        "\n",
        "embedding_dim = sample_embedding.shape[-1]\n",
        "print(f\"Embedding dimension: {embedding_dim}\")\n",
        "print(f\"Embedding shape: {sample_embedding.shape}\")\n",
        "\n",
        "# Check the SVM model's expected input dimension\n",
        "svm_input_dim = svm_model.support_vectors_.shape[1]\n",
        "print(f\"SVM model expects input dimension: {svm_input_dim}\")\n",
        "\n",
        "# Check if dimensions match\n",
        "if embedding_dim == svm_input_dim:\n",
        "    print(f\"✓ Embedding and SVM dimensions match! ({embedding_dim}D)\")\n",
        "else:\n",
        "    print(f\"✗ WARNING: Dimension mismatch! Embedding: {embedding_dim}D, SVM expects: {svm_input_dim}D\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Saving SVM Model...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Save the SVM model and label encoder\n",
        "import pickle\n",
        "\n",
        "model_data = {\n",
        "    'svm_model': svm_model,\n",
        "    'label_encoder': label_encoder,\n",
        "    'embedding_dim': embedding_dim\n",
        "}\n",
        "\n",
        "model_path = 'face_recognition_svm_model.pkl'\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(f\"✓ Model saved to: {model_path}\")\n",
        "print(f\"  - SVM classifier\")\n",
        "print(f\"  - Label encoder\")\n",
        "print(f\"  - Embedding dimension: {embedding_dim}\")\n",
        "print(\"\\nYou can now load this model for inference without retraining.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "FaceNet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f33d86320184170b812f61dc6265702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcd4c2d6967e4da58bc7990237dd5804",
              "IPY_MODEL_87a1b361715a483d9c563b323e95b4ce",
              "IPY_MODEL_cbd9d041c87a4eb8980c7b42a46a8dd1"
            ],
            "layout": "IPY_MODEL_30ef87bc803a4362b1666641f7101519"
          }
        },
        "30ef87bc803a4362b1666641f7101519": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a779b0648ef44bbbd3b073849cea617": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "589f696b730d4b8bbec70d67fa319b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ce610988fea45c7824d829f1f110a76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a1b361715a483d9c563b323e95b4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a779b0648ef44bbbd3b073849cea617",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90082bc963214a5c8e4f823e173a6265",
            "value": 111898327
          }
        },
        "90082bc963214a5c8e4f823e173a6265": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a10ba5cc249a44649405db59b212ae7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd9d041c87a4eb8980c7b42a46a8dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ce610988fea45c7824d829f1f110a76",
            "placeholder": "​",
            "style": "IPY_MODEL_589f696b730d4b8bbec70d67fa319b96",
            "value": " 107M/107M [00:00&lt;00:00, 207MB/s]"
          }
        },
        "dcd4c2d6967e4da58bc7990237dd5804": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a10ba5cc249a44649405db59b212ae7f",
            "placeholder": "​",
            "style": "IPY_MODEL_e66df7252a2a4992b86dbb9aca693fcd",
            "value": "100%"
          }
        },
        "e66df7252a2a4992b86dbb9aca693fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
